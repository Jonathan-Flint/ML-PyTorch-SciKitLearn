{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chapter 2: Training Simple Machine Learning Algorithms for Classification\n",
    "\n",
    "We will use 2 of the first described ML algorithms, the perceptron and adaptive linear neaurons. We will implement a perceptron to classify the flowers in the Iris dataset into species classes. Basics of optimization will then be explored using adaptive linear neaurons to lay the ground work for more sophisticated classifiers provided by scikit-learn.\n",
    "\n",
    "### Formal definition of an artificial neauron\n",
    "\n",
    "We put the idea of an artificial neauron into context using a binary classifcation where there are 2 classes, 0 and 1. We define a decision function $\\sigma (z)$ that takes a linear combination of certain input values $x$, and a corrsonding weight vector $w$ where $z$ is the so called net input $z = w_1x_1 + w_2x_2 + ... + w_mx_m$\n",
    "\n",
    "\\begin{equation}\n",
    "    w = \\begin{bmatrix} W_1 \n",
    "        \\\\ . \n",
    "        \\\\ . \n",
    "        \\\\ . \n",
    "        \\\\ W_m \n",
    "        \\end{bmatrix} , x = \\begin{bmatrix} X_1 \\\\ . \\\\ . \\\\ . \\\\ X_m \\end{bmatrix}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If the net input of a particular example $x^{i}$ is greater than the defined threshold $\\theta$ we predict the class 1, otherwise 0. In the perceptron algorithm the decision function $\\sigma (.)$ is a variant of the unit step function:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sigma (z) = \\begin{cases}\n",
    "        1 & if\\ z >= \\theta \\\\\n",
    "        0 & otherwise\n",
    "        \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "To simplify code implimentation later we modify this sllightly by bringing $\\theta$ across to the left:\n",
    "$$\n",
    "    z >= \\theta \\\\ z - \\theta >= 0\n",
    "$$\n",
    "Then we define a unit bias $b = -\\theta$ and include it in the net input:\n",
    "$$\n",
    "z = w_1x_1 + ... + w_mx_m + b = \\bf{w}^{T}\\bf{x}+b\n",
    "$$\n",
    "Third, given the introduction of the unit bias and redefinition of the net input $z$ above we can redefine the decision function as follows:\n",
    "$$\n",
    "\\sigma (z) = \\begin{cases} 1 & if\\ z>=0\\\\0 & otherwise \\end{cases}\n",
    "$$\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "<img src=\"Figures\\02_02.png\" width=\"800\" height=\"400\">\n",
    "</p>\n",
    "<p style=\"text-align: center;\"><b>Figure 2.2: A thrshold function that produces a linear decision boundary for a binary classification</b></p>\n",
    "\n",
    "Figure 2.2 shows how the net input $z = \\bf{w}^{T}\\bf{x} + b$ is swuashed into a binary 1 or 0 output by the decision function of the perceptron (left) and how it can be used to discriminate between 2 classes seperated by a linear decision boundary (right subfigure).\n",
    "\n",
    "### The perceptron learning rule\n",
    "\n",
    "The whole idea is to use a reductionist approach to mimic how a single neauron in the brain works, it either fires or it doesn't. The perceptron algorithm can be summerized by the following steps\n",
    "\n",
    "1. Initialise the weights and bias unit to 0 or small random numbers\n",
    "2. For each training example, $\\bf{x}^{(i)}$:\n",
    " * Compute the output value, $\\hat{y}^{(i)}$\n",
    " * Update the weight and bias units\n",
    "\n",
    " Here the output value is the class label predicted by the Heviside function and the simultaneous update of the bias unit and each weight can be formally written as:\n",
    "\n",
    "$$\n",
    "w_j := w_j + \\Delta w_j \\\\ b := b + \\Delta b\n",
    "$$\n",
    "The update values \"deltas\" are computed as:\n",
    "$$\n",
    "\\Delta w_j = \\eta(y^{(i)} - \\hat{y}^{(i)})x_j^{(i)}\\\\\n",
    "\\Delta b = \\eta(y^{(i)} - \\hat{y}^{(i)})\n",
    "$$\n",
    "Each weight is associated with a feature $x_j& in the dataset which is used in calculating the correction to $w_j$. $\\eta$ is the learning rate, typically a value $\\epsilon [0.0, 1.0]$, $y^{(i)}$ is the true class label of the ith training example and $\\hat{y^{(i)}}$ is the predicted class. All the updates are done simultaneously meaining we dont recompute the predicted label before the bias unit and all weights are updated.\n",
    "\n",
    "The convergence of the perceptron is not guarenteed however, if the two classes are not linearly seperable then a clean straight line cannot be drawn between them and the weights and biases will not converge to a constant value because there will always be some training examples that get mislabelled, and therefore the weights and biases will always get updated by some non-zero value. Figure 2.3 shows this diagramatically.\n",
    "\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "<img src=\"Figures\\02_03.png\" width=\"800\" height=\"400\">\n",
    "</p>\n",
    "<p style=\"text-align: center;\"><b>Figure 2.3: Example of a linearly seperable problem and 2 non-linearly seperable problems</b></p>\n",
    "\n",
    "If 2 classes are not linearly seperable then we can define either a max number of epochs to iterate and train over or a max number of acceptable mislabelled training examples, doing so for non-linearly seperable classes will ensure that the algorithm will converge or cease training even if it sees there are still some mislabelled training examples. We will later see an example algorithm call the Adaline algorithm which achieves convergence even when the dataset looks like subfigures 2.3a or 2.3b. There are more advanced algorithms which can produce non-linear decision boundaries which will also be covered later.\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "<img src=\"Figures\\02_04.png\" width=\"800\" height=\"400\">\n",
    "</p>\n",
    "<p style=\"text-align: center;\"><b>Figure 2.4: Summary of the learning process for the perceptron</b></p>\n",
    "\n",
    "### Implementing the perceptron in python 3.9\n",
    "\n",
    "We will implement a perceptron tailored around the iris dataset touched upon in chapter 1.\n",
    "\n",
    "### An object oriented perceptron API\n",
    "\n",
    "We will define the perceptron interface as a python class which allows new perceptrons objects to be initialised that can learn from the data via a fit method and make predictions via a seperate predict method. As a convention we append an underscore (\\_) ti attributes that are not created upon initialisation of the object, but we do this by calling the objects other methods, for example self.w_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perc:\n",
    "    \"\"\"\n",
    "    Perceptron Classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eta : float\n",
    "        Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "        number of passes over the training dataset\n",
    "    random_state: int\n",
    "        Random number generation seed for random weight initialisation\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    w_ : 1d-array\n",
    "        Weights after fitting\n",
    "    b_ : Scalar\n",
    "        Bias unit after fitting\n",
    "    \n",
    "    errors_ : list\n",
    "        Number of misclassifications (updates) in each epoch\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_examples, n_features]\n",
    "            Training vectors, where n_examples is the number of examples and n_features is the number of features\n",
    "        y : array-like, shape = [n_examples]\n",
    "            Target values\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "\n",
    "        \"\"\"\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])\n",
    "        self.b_ = np.float_(0.)\n",
    "        self.errors_ = []\n",
    "        for i in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                update = self.eta * (target - self.predict(xi))\n",
    "                self.w_ += update * xi\n",
    "                self.b_ += update\n",
    "                errors += int(update != 0.0)\n",
    "            self.errors_.append(errors)\n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate the net input\"\"\"\n",
    "        return np.dot(X, self.w_) + self.b_\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
